{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with SageMaker's Prebuilt Deep Learning Containers\n",
    "\n",
    "In this module, we'll see how to train and test a Sentiment Analysis (Text Classification) model on SageMaker using SageMaker's Prebuilt Deep Learning containers.  These containers are available for TensorFlow, MXNet, PyTorch, and Chainer.  With this approach, you simply bring your own Python training script, and SageMaker handles the rest.  \n",
    "\n",
    "We'll begin by importing some necessary libraries and downloading the Python training script.  The IAM role needed for permissions, such as access to data in Amazon S3, is pulled in from the SageMaker Notebook Instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"aws s3 cp s3://sagemaker-workshop-pdx/sentiment-analysis-module/sentiment.py sentiment.py\")\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train the **Sentiment Analysis** model on [SST-2 dataset (Stanford Sentiment Treebank 2)](https://nlp.stanford.edu/sentiment/index.html). The dataset consists of movie reviews with one sentence per review. Classification involves detecting positive/negative reviews.  \n",
    "We will download the preprocessed version of this dataset from the links below. Each line in the dataset has space separated tokens, the first token being the label: 1 for positive and 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      " 65 4147k   65 2735k    0     0  6188k      0 --:--:-- --:--:-- --:--:-- 6174k\r",
      "100 4147k  100 4147k    0     0  8413k      0 --:--:-- --:--:-- --:--:-- 8396k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100  189k  100  189k    0     0   900k      0 --:--:-- --:--:-- --:--:--  900k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir data\n",
    "curl https://raw.githubusercontent.com/saurabh3949/Text-Classification-Datasets/master/stsa.binary.phrases.train > data/train\n",
    "curl https://raw.githubusercontent.com/saurabh3949/Text-Classification-Datasets/master/stsa.binary.test > data/test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the data\n",
    "\n",
    "We use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value `inputs` identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training function\n",
    "\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a `train` function. When SageMaker calls your function, it will pass in arguments that describe the training environment. Check the script below to see how this works.\n",
    "\n",
    "The script here is a simplified implementation of [\"Bag of Tricks for Efficient Text Classification\"](https://arxiv.org/abs/1607.01759), as implemented by Facebook's [FastText](https://github.com/facebookresearch/fastText/) for text classification. The model maps each word to a vector and averages vectors of all the words in a sentence to form a hidden representation of the sentence, which is inputted to a softmax classification layer. Please refer to the paper for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd, nd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "import re\r\n",
      "from mxnet.io import DataIter, DataBatch, DataDesc\r\n",
      "import bisect, random\r\n",
      "from collections import Counter\r\n",
      "from itertools import chain, islice\r\n",
      "\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def train(current_host, hosts, num_cpus, num_gpus, channel_input_dirs, model_dir, hyperparameters, **kwargs):\r\n",
      "    # retrieve the hyperparameters we set in notebook (with some defaults)\r\n",
      "    batch_size = hyperparameters.get('batch_size', 8)\r\n",
      "    epochs = hyperparameters.get('epochs', 2)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.01)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 1000)\r\n",
      "    embedding_size = hyperparameters.get('embedding_size', 50)\r\n",
      "\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "\r\n",
      "    ctx = mx.gpu() if num_gpus > 0 else mx.cpu()\r\n",
      "\r\n",
      "    training_dir = channel_input_dirs['training']\r\n",
      "    train_sentences, train_labels, _ = get_dataset(training_dir + '/train')\r\n",
      "    val_sentences, val_labels, _ = get_dataset(training_dir + '/test')\r\n",
      "\r\n",
      "    num_classes = len(set(train_labels))\r\n",
      "    vocab = create_vocab(train_sentences)\r\n",
      "    vocab_size = len(vocab)\r\n",
      "\r\n",
      "    train_sentences = [[vocab.get(token, 1) for token in line if len(line)>0] for line in train_sentences]\r\n",
      "    val_sentences = [[vocab.get(token, 1) for token in line if len(line)>0] for line in val_sentences]\r\n",
      "\r\n",
      "    # Alternatively to splitting in memory, the data could be pre-split in S3 and use ShardedByS3Key\r\n",
      "    # to do parallel training.\r\n",
      "    shard_size = len(train_sentences) // len(hosts)\r\n",
      "    for i, host in enumerate(hosts):\r\n",
      "        if host == current_host:\r\n",
      "            start = shard_size * i\r\n",
      "            end = start + shard_size\r\n",
      "            break\r\n",
      "\r\n",
      "    train_iterator = BucketSentenceIter(train_sentences[start:end], train_labels[start:end], batch_size)\r\n",
      "    val_iterator = BucketSentenceIter(val_sentences, val_labels, batch_size)\r\n",
      "\r\n",
      "    # define the network\r\n",
      "    net = TextClassifier(vocab_size, embedding_size, num_classes)\r\n",
      "\r\n",
      "    # Collect all parameters from net and its children, then initialize them.\r\n",
      "    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\r\n",
      "    # Trainer is for updating parameters with gradient.\r\n",
      "    trainer = gluon.Trainer(net.collect_params(), 'adam',\r\n",
      "                            {'learning_rate': learning_rate},\r\n",
      "                            kvstore=kvstore)\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\r\n",
      "\r\n",
      "    for epoch in range(epochs):\r\n",
      "        # reset data iterator and metric at begining of epoch.\r\n",
      "        metric.reset()\r\n",
      "        btic = time.time()\r\n",
      "        i = 0\r\n",
      "        for batch in train_iterator:\r\n",
      "            # Copy data to ctx if necessary\r\n",
      "            data = batch.data[0].as_in_context(ctx)\r\n",
      "            label = batch.label[0].as_in_context(ctx)\r\n",
      "\r\n",
      "            # Start recording computation graph with record() section.\r\n",
      "            # Recorded graphs can then be differentiated with backward.\r\n",
      "            with autograd.record():\r\n",
      "                output = net(data)\r\n",
      "                L = loss(output, label)\r\n",
      "                L.backward()\r\n",
      "            # take a gradient step with batch_size equal to data.shape[0]\r\n",
      "            trainer.step(data.shape[0])\r\n",
      "            # update metric at last.\r\n",
      "            metric.update([label], [output])\r\n",
      "\r\n",
      "            if i % log_interval == 0 and i > 0:\r\n",
      "                name, acc = metric.get()\r\n",
      "                print('[Epoch %d Batch %d] Training: %s=%f, %f samples/s' %\r\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\r\n",
      "\r\n",
      "            btic = time.time()\r\n",
      "            i += 1\r\n",
      "\r\n",
      "        name, acc = metric.get()\r\n",
      "        print('[Epoch %d] Training: %s=%f' % (epoch, name, acc))\r\n",
      "\r\n",
      "        name, val_acc = test(ctx, net, val_iterator)\r\n",
      "        print('[Epoch %d] Validation: %s=%f' % (epoch, name, val_acc))\r\n",
      "        train_iterator.reset()\r\n",
      "    return net, vocab\r\n",
      "\r\n",
      "\r\n",
      "class BucketSentenceIter(DataIter):\r\n",
      "    \"\"\"Simple bucketing iterator for text classification model.\r\n",
      "    Parameters\r\n",
      "    ----------\r\n",
      "    sentences : list of list of int\r\n",
      "        Encoded sentences.\r\n",
      "    labels : list of int\r\n",
      "        Corresponding labels.\r\n",
      "    batch_size : int\r\n",
      "        Batch size of the data.\r\n",
      "    buckets : list of int, optional\r\n",
      "        Size of the data buckets. Automatically generated if None.\r\n",
      "    invalid_label : int, optional\r\n",
      "        Key for invalid label, e.g. <unk. The default is 0.\r\n",
      "    dtype : str, optional\r\n",
      "        Data type of the encoding. The default data type is 'float32'.\r\n",
      "    data_name : str, optional\r\n",
      "        Name of the data. The default name is 'data'.\r\n",
      "    label_name : str, optional\r\n",
      "        Name of the label. The default name is 'softmax_label'.\r\n",
      "    layout : str, optional\r\n",
      "        Format of data and label. 'NT' means (batch_size, length)\r\n",
      "        and 'TN' means (length, batch_size).\r\n",
      "    \"\"\"\r\n",
      "    def __init__(self, sentences, labels, batch_size, buckets=None, invalid_label=0,\r\n",
      "                 data_name='data', label_name='softmax_label', dtype='float32',\r\n",
      "                 layout='NT'):\r\n",
      "        super(BucketSentenceIter, self).__init__()\r\n",
      "        if not buckets:\r\n",
      "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\r\n",
      "                       if j >= batch_size]\r\n",
      "        buckets.sort()\r\n",
      "\r\n",
      "        ndiscard = 0\r\n",
      "        self.data = [[] for _ in buckets]\r\n",
      "        self.labels = [[] for _ in buckets]\r\n",
      "        for i, sent in enumerate(sentences):\r\n",
      "            buck = bisect.bisect_left(buckets, len(sent))\r\n",
      "            if buck == len(buckets):\r\n",
      "                ndiscard += 1\r\n",
      "                continue\r\n",
      "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\r\n",
      "            buff[:len(sent)] = sent\r\n",
      "            self.data[buck].append(buff)\r\n",
      "            self.labels[buck].append(labels[i])\r\n",
      "\r\n",
      "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\r\n",
      "        self.labels = [np.asarray(i, dtype=dtype) for i in self.labels]\r\n",
      "\r\n",
      "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\r\n",
      "\r\n",
      "        self.batch_size = batch_size\r\n",
      "        self.buckets = buckets\r\n",
      "        self.data_name = data_name\r\n",
      "        self.label_name = label_name\r\n",
      "        self.dtype = dtype\r\n",
      "        self.invalid_label = invalid_label\r\n",
      "        self.nddata = []\r\n",
      "        self.ndlabel = []\r\n",
      "        self.major_axis = layout.find('N')\r\n",
      "        self.layout = layout\r\n",
      "        self.default_bucket_key = max(buckets)\r\n",
      "\r\n",
      "        if self.major_axis == 0:\r\n",
      "            self.provide_data = [DataDesc(\r\n",
      "                name=self.data_name, shape=(batch_size, self.default_bucket_key),\r\n",
      "                layout=self.layout)]\r\n",
      "            self.provide_label = [DataDesc(\r\n",
      "                name=self.label_name, shape=(batch_size,),\r\n",
      "                layout=self.layout)]\r\n",
      "        elif self.major_axis == 1:\r\n",
      "            self.provide_data = [DataDesc(\r\n",
      "                name=self.data_name, shape=(self.default_bucket_key, batch_size),\r\n",
      "                layout=self.layout)]\r\n",
      "            self.provide_label = [DataDesc(\r\n",
      "                name=self.label_name, shape=(self.default_bucket_key, batch_size),\r\n",
      "                layout=self.layout)]\r\n",
      "        else:\r\n",
      "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\r\n",
      "\r\n",
      "        self.idx = []\r\n",
      "        for i, buck in enumerate(self.data):\r\n",
      "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\r\n",
      "        self.curr_idx = 0\r\n",
      "        self.reset()\r\n",
      "\r\n",
      "    def reset(self):\r\n",
      "        \"\"\"Resets the iterator to the beginning of the data.\"\"\"\r\n",
      "        self.curr_idx = 0\r\n",
      "        random.shuffle(self.idx)\r\n",
      "        for i in range(len(self.data)):\r\n",
      "            data, labels =  self.data[i], self.labels[i]\r\n",
      "            p = np.random.permutation(len(data))\r\n",
      "            self.data[i], self.labels[i] = data[p], labels[p]\r\n",
      "\r\n",
      "        self.nddata = []\r\n",
      "        self.ndlabel = []\r\n",
      "        for buck,label_buck in zip(self.data, self.labels):\r\n",
      "            self.nddata.append(nd.array(buck, dtype=self.dtype))\r\n",
      "            self.ndlabel.append(nd.array(label_buck, dtype=self.dtype))\r\n",
      "\r\n",
      "    def next(self):\r\n",
      "        \"\"\"Returns the next batch of data.\"\"\"\r\n",
      "        if self.curr_idx == len(self.idx):\r\n",
      "            raise StopIteration\r\n",
      "        i, j = self.idx[self.curr_idx]\r\n",
      "        self.curr_idx += 1\r\n",
      "\r\n",
      "        if self.major_axis == 1:\r\n",
      "            data = self.nddata[i][j:j+self.batch_size].T\r\n",
      "            label = self.ndlabel[i][j:j+self.batch_size].T\r\n",
      "        else:\r\n",
      "            data = self.nddata[i][j:j+self.batch_size]\r\n",
      "            label = self.ndlabel[i][j:j+self.batch_size]\r\n",
      "\r\n",
      "        return DataBatch([data], [label], pad=0,\r\n",
      "                         bucket_key=self.buckets[i],\r\n",
      "                         provide_data=[DataDesc(\r\n",
      "                             name=self.data_name, shape=data.shape,\r\n",
      "                             layout=self.layout)],\r\n",
      "                         provide_label=[DataDesc(\r\n",
      "                             name=self.label_name, shape=label.shape,\r\n",
      "                             layout=self.layout)])\r\n",
      "\r\n",
      "\r\n",
      "class TextClassifier(gluon.HybridBlock):\r\n",
      "    def __init__(self, vocab_size, embedding_size, classes, **kwargs):\r\n",
      "        super(TextClassifier, self).__init__(**kwargs)\r\n",
      "        with self.name_scope():\r\n",
      "            self.dense = gluon.nn.Dense(classes)\r\n",
      "            self.embedding = gluon.nn.Embedding(input_dim=vocab_size, output_dim=embedding_size)\r\n",
      "\r\n",
      "    def hybrid_forward(self, F, x):\r\n",
      "        x = self.embedding(x)\r\n",
      "        x = F.mean(x, axis=1)\r\n",
      "        x = self.dense(x)\r\n",
      "        return x\r\n",
      "\r\n",
      "\r\n",
      "def get_dataset(filename):\r\n",
      "    labels = []\r\n",
      "    sentences = []\r\n",
      "    max_length = -1\r\n",
      "    with open(filename) as f:\r\n",
      "        for line in f:\r\n",
      "            tokens = line.split()\r\n",
      "            label = int(tokens[0])\r\n",
      "            words = tokens[1:]\r\n",
      "            max_length = max(max_length, len(words))\r\n",
      "            labels.append(label)\r\n",
      "            sentences.append(words)\r\n",
      "    return sentences, labels, max_length\r\n",
      "\r\n",
      "\r\n",
      "def create_vocab(sentences, min_count=5, num_words = 100000):\r\n",
      "    BOS_SYMBOL = \"<s>\"\r\n",
      "    EOS_SYMBOL = \"</s>\"\r\n",
      "    UNK_SYMBOL = \"<unk>\"\r\n",
      "    PAD_SYMBOL = \"<pad>\"\r\n",
      "    PAD_ID = 0\r\n",
      "    TOKEN_SEPARATOR = \" \"\r\n",
      "    VOCAB_SYMBOLS = [PAD_SYMBOL, UNK_SYMBOL, BOS_SYMBOL, EOS_SYMBOL]\r\n",
      "    VOCAB_ENCODING = \"utf-8\"\r\n",
      "    vocab_symbols_set = set(VOCAB_SYMBOLS)\r\n",
      "    raw_vocab = Counter(token for line in sentences for token in line)\r\n",
      "    pruned_vocab = sorted(((c, w) for w, c in raw_vocab.items() if c >= min_count), reverse=True)\r\n",
      "    vocab = islice((w for c, w in pruned_vocab), num_words)\r\n",
      "    word_to_id = {word: idx for idx, word in enumerate(chain(VOCAB_SYMBOLS, vocab))}\r\n",
      "    return word_to_id\r\n",
      "\r\n",
      "\r\n",
      "def vocab_to_json(vocab, path):\r\n",
      "    with open(path, \"w\") as out:\r\n",
      "        json.dump(vocab, out, indent=4, ensure_ascii=True)\r\n",
      "        print('Vocabulary saved to \"%s\"', path)\r\n",
      "\r\n",
      "\r\n",
      "def vocab_from_json(path):\r\n",
      "    with open(path) as inp:\r\n",
      "        vocab = json.load(inp)\r\n",
      "        print('Vocabulary (%d words) loaded from \"%s\"', len(vocab), path)\r\n",
      "        return vocab\r\n",
      "\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    # save the model\r\n",
      "    net, vocab = net\r\n",
      "    y = net(mx.sym.var('data'))\r\n",
      "    y.save('%s/model.json' % model_dir)\r\n",
      "    net.collect_params().save('%s/model.params' % model_dir)\r\n",
      "    vocab_to_json(vocab, '%s/vocab.json' % model_dir)\r\n",
      "\r\n",
      "\r\n",
      "def test(ctx, net, val_data):\r\n",
      "    val_data.reset()\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    for batch in val_data:\r\n",
      "        data = batch.data[0].as_in_context(ctx)\r\n",
      "        label = batch.label[0].as_in_context(ctx)\r\n",
      "        output = net(data)\r\n",
      "        metric.update([label], [output])\r\n",
      "    return metric.get()\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    symbol = mx.sym.load('%s/model.json' % model_dir)\r\n",
      "    vocab = vocab_from_json('%s/vocab.json' % model_dir)\r\n",
      "    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\r\n",
      "    inputs = mx.sym.var('data')\r\n",
      "    param_dict = gluon.ParameterDict('model_')\r\n",
      "    net = gluon.SymbolBlock(outputs, inputs, param_dict)\r\n",
      "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\r\n",
      "    return net, vocab\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    # we can use content types to vary input/output handling, but\r\n",
      "    # here we just assume json for both\r\n",
      "    net, vocab = net\r\n",
      "    parsed = json.loads(data)\r\n",
      "    outputs = []\r\n",
      "    for row in parsed:\r\n",
      "        tokens = [vocab.get(token, 1) for token in row.split()]\r\n",
      "        nda = mx.nd.array([tokens])\r\n",
      "        output = net(nda)\r\n",
      "        prediction = mx.nd.argmax(output, axis=1)\r\n",
      "        outputs.append(int(prediction.asscalar()))\r\n",
      "    response_body = json.dumps(outputs)\r\n",
      "    return response_body, output_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'sentiment.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training script on SageMaker\n",
    "\n",
    "To keep our code readable and concise, we'll set up a training job using the SageMaker Python SDK, which provides many helper methods and conveniences. The SDK provides a specific Estimator objects for various frameworks that abstract away the lower level details of setting up training jobs. Various hyperparameters can be specified, including the learning rate etc.  You also can specify the type and amount of training hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MXNet(\"sentiment.py\",\n",
    "          role=role,\n",
    "          train_instance_count=1,\n",
    "          train_instance_type=\"ml.c5.9xlarge\",\n",
    "          framework_version=\"1.2.1\",\n",
    "          hyperparameters={'batch_size': 8,\n",
    "                         'epochs': 2,\n",
    "                         'learning_rate': 0.01,\n",
    "                         'embedding_size': 50, \n",
    "                         'log_interval': 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our Estimator object, we can fit it to the training data we uploaded to S3. In this case, we're using the default training File mode; SageMaker makes sure our data is available in the training cluster's filesystem, so our training script can simply read the data from disk.  An alternative is Pipe mode, where the data is streamed directly to the container without being persisted to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-09-21-17-46-12-593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "\u001b[31m2018-09-21 17:48:28,694 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-09-21 17:48:28,694 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-09-21 17:48:28,699 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31m2018-09-21 17:48:29,908 WARNING - mxnet_container.train - #033[1;33mThis required structure for training scripts will be deprecated with the next major release of MXNet images. The train() function will no longer be required; instead the training script must be able to be run as a standalone script. For more information, see https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/mxnet#updating-your-mxnet-training-script.#033[1;0m\u001b[0m\n",
      "\u001b[31m2018-09-21 17:48:30,517 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'enable_cloudwatch_metrics': False, 'available_gpus': 0, 'channels': {u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, '_ps_verbose': 0, 'resource_config': {u'hosts': [u'algo-1'], u'network_interface_name': u'ethwe', u'current_host': u'algo-1'}, 'user_script_name': u'sentiment.py', 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {u'training': u'/opt/ml/input/data/training'}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'output_dir': '/opt/ml/output', 'model_dir': '/opt/ml/model', 'hyperparameters': {u'sagemaker_program': u'sentiment.py', u'embedding_size': 50, u'learning_rate': 0.01, u'log_interval': 1000, u'epochs': 2, u'batch_size': 8, u'sagemaker_region': u'us-west-2', u'sagemaker_enable_cloudwatch_metrics': False, u'sagemaker_job_name': u'sagemaker-mxnet-2018-09-21-17-46-12-593', u'sagemaker_container_log_level': 20, u'sagemaker_submit_directory': u's3://sagemaker-us-west-2-894087409521/sagemaker-mxnet-2018-09-21-17-46-12-593/source/sourcedir.tar.gz'}, 'hosts': [u'algo-1'], 'job_name': 'sagemaker-mxnet-2018-09-21-17-46-12-593', '_ps_port': 8000, 'user_script_archive': u's3://sagemaker-us-west-2-894087409521/sagemaker-mxnet-2018-09-21-17-46-12-593/source/sourcedir.tar.gz', '_scheduler_host': u'algo-1', 'sagemaker_region': u'us-west-2', '_scheduler_ip': '10.32.0.4', 'input_dir': '/opt/ml/input', 'user_requirements_file': None, 'current_host': u'algo-1', 'container_log_level': 20, 'available_cpus': 36, 'base_dir': '/opt/ml'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-west-2-894087409521/sagemaker-mxnet-2018-09-21-17-46-12-593/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-09-21 17:48:39,047 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31mWARNING: discarded 9 sentences longer than the largest bucket.\u001b[0m\n",
      "\u001b[31mWARNING: discarded 27 sentences longer than the largest bucket.\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 1000] Training: accuracy=0.738012, 3620.070342 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 2000] Training: accuracy=0.777924, 2537.964753 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 3000] Training: accuracy=0.800192, 2501.635130 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 4000] Training: accuracy=0.813828, 2419.034821 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 5000] Training: accuracy=0.822486, 2672.808029 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 6000] Training: accuracy=0.829716, 2242.044100 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 7000] Training: accuracy=0.835381, 2294.947815 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 8000] Training: accuracy=0.840004, 2921.841867 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 9000] Training: accuracy=0.844656, 2656.934991 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0] Training: accuracy=0.846488\u001b[0m\n",
      "\u001b[31m[Epoch 0] Validation: accuracy=0.827153\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 1000] Training: accuracy=0.905470, 2455.501793 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 2000] Training: accuracy=0.902111, 2530.881883 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 3000] Training: accuracy=0.898326, 2596.489360 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 4000] Training: accuracy=0.898119, 2822.783882 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 5000] Training: accuracy=0.897620, 3088.873424 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 6000] Training: accuracy=0.897309, 2751.039764 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 7000] Training: accuracy=0.897836, 2722.910979 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 8000] Training: accuracy=0.895951, 2698.169186 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 9000] Training: accuracy=0.896025, 2555.943937 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1] Training: accuracy=0.895827\u001b[0m\n",
      "\u001b[31m[Epoch 1] Validation: accuracy=0.816986\u001b[0m\n",
      "\u001b[31mVocabulary saved to \"%s\" /opt/ml/model/vocab.json\u001b[0m\n",
      "\n",
      "Billable seconds: 89\n"
     ]
    }
   ],
   "source": [
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the logs, we get > 80% validation accuracy on the test set using the above hyperparameters after only two epochs (passes over the full training set).  \n",
    "\n",
    "After training, we use the Estimator object to build and deploy an Predictor object. This creates a SageMaker endpoint that we can use to perform inference. In fact, we'll be able to perform inference on a standard JSON encoded string array without having to use any special encoding formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-mxnet-2018-09-21-17-46-12-593\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-mxnet-2018-09-21-17-46-12-593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor runs inference on our input data and returns the predicted sentiment (1 for positive and 0 for negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "data = [\"this movie was extremely good .\",\n",
    "        \"the plot was very boring .\",\n",
    "        \"this film is so slick , superficial and trend-hoppy .\",\n",
    "        \"i just could not watch it till the end .\",\n",
    "        \"the movie was so enthralling !\"]\n",
    "\n",
    "response = predictor.predict(data)\n",
    "print response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Cleanup\n",
    "\n",
    "You are now done with this module!  Return to the workshop lab guide whenever you're ready and continue with the next module(s).  \n",
    "\n",
    "Remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-mxnet-2018-09-21-17-46-12-593\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
